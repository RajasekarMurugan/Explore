{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAR2hZPNhsTK/T9qWAoTsL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajasekarMurugan/Explore/blob/main/Text_Processing_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Text processing using NLTK (Natural **Lanaguage** toolkit)"
      ],
      "metadata": {
        "id": "HvJ9VigT-GGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAb06aIl-DD2",
        "outputId": "2550be1a-a49d-457f-d5bb-1e18a745bb63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import  PorterStemmer, WordNetLemmatizer\n",
        "from gensim.models import word2vec\n"
      ],
      "metadata": {
        "id": "YQqFafUJ-ilC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")  # most common words in any given language\n",
        "nltk.download(\"wordnet\")  # Dictionary of a given lanaguage (word and its synonyms, antonyms, etc..)\n",
        "nltk.download(\"punkt_tab\")  #  Pre-trained unpervised tokenizer\n",
        "nltk.download(\"omw-1.4\")  # Open multi-lingual wordnet (multi-lingual version of wordnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9vsg62p-6ei",
        "outputId": "b46725e9-2710-479a-f395-541ecf10c731"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "\n",
        "sentences = [\"The students today are learning about natural lanaguage Processing in Artificial Intelligence\"]"
      ],
      "metadata": {
        "id": "mc6LDZh3_uVV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "print(\"Tokennized: \", tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmUmwm0lFYkO",
        "outputId": "22d4ead2-44e8-4a41-a487-094c7de018af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokennized:  [['the', 'students', 'today', 'are', 'learning', 'about', 'natural', 'lanaguage', 'processing', 'in', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords removal (Keep only the keywords)\n",
        "stop_words = set(stopwords.words(\"english\")) # setting the stopwords to english\n",
        "\n",
        "\n",
        "filtered_sentences = [\n",
        "    [word for word in sentence if word.isalnum() and word not in stop_words]\n",
        "    for sentence in tokenized_sentences\n",
        "]\n",
        "print(\"After stopword removal :\", filtered_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajp-11rMF2UD",
        "outputId": "9d5d46eb-730d-417b-fe79-295c8c400c53"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stopword removal : [['students', 'today', 'learning', 'natural', 'lanaguage', 'processing', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming - identify root word\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_sentences = [\n",
        "    [stemmer.stem(word) for word in sentence]\n",
        "    for sentence in filtered_sentences\n",
        "]\n",
        "print(\"Stemmed sentence\", stemmed_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aueYf7zqNIwB",
        "outputId": "759f08fd-644e-437a-c7e9-b5e1f9572f9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed sentence [['student', 'today', 'learn', 'natur', 'lanaguag', 'process', 'artifici', 'intellig']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmetization  - identify meaningful root word\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_sentences=[\n",
        "    [lemmatizer.lemmatize(word) for word in sentence]\n",
        "    for sentence in filtered_sentences\n",
        "]\n",
        "print(\"Lemmatized sentences \", lemmatized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kf9MV6fIyd6",
        "outputId": "256ec1cb-1864-4d9b-fd31-71b4d3ae7b8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized sentences  [['student', 'today', 'learning', 'natural', 'lanaguage', 'processing', 'artificial', 'intelligence']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to text to numbers (vectors)\n",
        "\n",
        "model = Word2Vec(sentences=lemmatized_sentences, vector_size=4, window=1, min_count=1, workers=4)\n",
        "# Vector Size is the dimension of the output vector generated\n",
        "# window si th number of words that we consider after/before the query word to understand the meaning\n",
        "\n",
        "\n",
        "print(\"Vector for the word 'artificial':\", model.wv['artificial'])\n",
        "print(\"Vector for the word 'student':\", model.wv['student'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fs6IhS1M-JA",
        "outputId": "a0cf98f3-428f-4d69-c429-000d621a877c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for the word 'artificial': [-0.23257375 -0.17792022  0.16147181  0.2243247 ]\n",
            "Vector for the word 'student': [-0.18804094 -0.09840259 -0.18778956 -0.02325106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8r0HRz2EPLM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}